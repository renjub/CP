{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# File name containing the URLs\n",
    "file_name = 'bajaj_urls.csv'\n",
    "# Directory where the output files will be stored\n",
    "output_directory = 'output_files'\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Read URLs from the file\n",
    "with open(file_name, 'r') as file:\n",
    "    urls = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Function to fetch and parse a URL\n",
    "def fetch_and_parse(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to format and extract text, including bold inline\n",
    "def format_text(element):\n",
    "    # Replace bold tags (<b> and <strong>) with a markdown-style indication for bold\n",
    "    for bold in element.find_all(['strong', 'b']):\n",
    "        bold.string = f\"**{bold.get_text().strip()}**\"\n",
    "    return element.get_text().strip()\n",
    "\n",
    "# Function to extract and write content from a page\n",
    "def extract_content(soup, f_out):\n",
    "    # Try to find the specific container for the main content: <div class=\"post-content\">\n",
    "    container = soup.find('div', class_='post-content')\n",
    "    \n",
    "    # Check if the container was found\n",
    "    if container:\n",
    "        # Find all headings, paragraph elements, and <div class=\"text-yellow\">\n",
    "        elements = container.find_all(['h2', 'h3', 'h4', 'p', 'div'])\n",
    "        \n",
    "        # Iterate through each element and write to file\n",
    "        for element in elements:\n",
    "            # Only include <div> elements with class \"text-yellow\"\n",
    "            if element.name == 'div' and 'text-yellow' not in element.get('class', []):\n",
    "                continue\n",
    "\n",
    "            text = format_text(element)\n",
    "            if element.name in ['h2', 'h3', 'h4']:  # Headings\n",
    "                f_out.write(f\"\\n**{text}**\\n\")\n",
    "            elif element.name == 'p' or (element.name == 'div' and 'text-yellow' in element.get('class', [])):  # Paragraphs or text-yellow divs\n",
    "                f_out.write(f\"{text}\\n\")\n",
    "    else:\n",
    "        f_out.write(\"Main content container (<div class='post-content'>) not found.\\n\")\n",
    "\n",
    "# Function to extract all pagination links\n",
    "def get_pagination_links(soup):\n",
    "    pagination_links = []\n",
    "    page_links_container = soup.select_one('div.page-links')\n",
    "    \n",
    "    if page_links_container:\n",
    "        # Find all the anchor tags within the pagination container\n",
    "        page_links = page_links_container.find_all('a')\n",
    "        for link in page_links:\n",
    "            page_url = link.get('href')\n",
    "            if page_url and page_url not in pagination_links:\n",
    "                pagination_links.append(page_url)\n",
    "    \n",
    "    return pagination_links\n",
    "\n",
    "# Iterate through each URL and perform the operations\n",
    "for url in urls:\n",
    "    # Create a filename based on the URL\n",
    "    url_filename = url.split('/')[-2] if url.endswith('/') else url.split('/')[-1]\n",
    "    output_file = os.path.join(output_directory, f\"{url_filename}.txt\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(f\"URL: {url}\\n\\n\")\n",
    "        \n",
    "        # Fetch and parse the main page\n",
    "        soup = fetch_and_parse(url)\n",
    "        \n",
    "        if soup:\n",
    "            # Extract and write content from the main page\n",
    "            extract_content(soup, f_out)\n",
    "            \n",
    "            # Check if there are multiple pages\n",
    "            pagination_links = get_pagination_links(soup)\n",
    "            \n",
    "            # If pagination links are found, fetch and extract content from each page\n",
    "            for page_url in pagination_links:\n",
    "                f_out.write(f\"\\nProcessing additional page: {page_url}\\n\\n\")\n",
    "                page_soup = fetch_and_parse(page_url)\n",
    "                if page_soup:\n",
    "                    extract_content(page_soup, f_out)\n",
    "        else:\n",
    "            f_out.write(\"Failed to parse the main page.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
