{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bajaj Freedom https://www.bikewale.com/bajaj-bikes/cng-freedom-125/reviews/\n",
      "Found 10 reviews on the page.\n",
      "Saved 10 reviews to reviews.csv.\n",
      "Found 8 reviews on the page.\n",
      "Saved 8 reviews to reviews.csv.\n",
      "Found 3 reviews on the page.\n",
      "Saved 3 reviews to reviews.csv.\n",
      "Error in review processing: Page.wait_for_selector: Timeout 5000ms exceeded.\n",
      "Call log:\n",
      "waiting for locator(\"#user-review-header > div > div > div.o-brXWGL > div.o-dJmcbh > div > ul > li\") to be visible\n",
      "\n",
      "Error in review processing: Page.wait_for_selector: Timeout 5000ms exceeded.\n",
      "Call log:\n",
      "waiting for locator(\"#user-review-header > div > div > div.o-brXWGL > div.o-dJmcbh > div > ul > li\") to be visible\n",
      "\n",
      "No more pages for https://www.bikewale.com/bajaj-bikes/cng-freedom-125/reviews/.\n",
      "Bajaj Pulsar N160 https://www.bikewale.com/bajaj-bikes/pulsar-n160/reviews/\n",
      "Found 10 reviews on the page.\n",
      "Saved 10 reviews to reviews.csv.\n",
      "Found 8 reviews on the page.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 251\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished scraping all URLs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scrape_reviews()\n",
      "Cell \u001b[0;32mIn[1], line 225\u001b[0m, in \u001b[0;36mscrape_reviews\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;66;03m# Get and process reviews on the current page\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m         all_reviews \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_and_process_reviews(bike_name, page)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# Save the reviews to CSV after each page is processed\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m all_reviews:\n",
      "Cell \u001b[0;32mIn[1], line 104\u001b[0m, in \u001b[0;36mget_and_process_reviews\u001b[0;34m(bike_name, page)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Extract \"Review was not useful\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m not_useful_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m review_item\u001b[38;5;241m.\u001b[39mquery_selector(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv > div > div.o-cpnuEd.o-dsiSgT.o-bUVylL.o-fznJzb > div > div:nth-child(3) > p\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 104\u001b[0m not_useful_text \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m not_useful_element\u001b[38;5;241m.\u001b[39mtext_content())\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m not_useful_element \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Extract number of filled stars by checking the class \"o-vYvcJ\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m stars_container \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m review_item\u001b[38;5;241m.\u001b[39mquery_selector(\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv.o-fcaNGp.o-dsiSgT.o-NBTwp.o-dGBYL.o-bdcqQE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m )\n",
      "File \u001b[0;32m/datadrive/cp_env/lib/python3.8/site-packages/playwright/async_api/_generated.py:1700\u001b[0m, in \u001b[0;36mElementHandle.text_content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_content\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ElementHandle.text_content\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \n\u001b[1;32m   1693\u001b[0m \u001b[38;5;124;03m    Returns the `node.textContent`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m    Union[str, None]\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_maybe_impl(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mtext_content())\n",
      "File \u001b[0;32m/datadrive/cp_env/lib/python3.8/site-packages/playwright/_impl/_element_handle.py:75\u001b[0m, in \u001b[0;36mElementHandle.text_content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_content\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextContent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/datadrive/cp_env/lib/python3.8/site-packages/playwright/_impl/_connection.py:59\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m/datadrive/cp_env/lib/python3.8/site-packages/playwright/_impl/_connection.py:512\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(parsed_st)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/cp_env/lib/python3.8/site-packages/playwright/_impl/_connection.py:88\u001b[0m, in \u001b[0;36mChannel.inner_send\u001b[0;34m(self, method, params, return_as_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m     85\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_send_message_to_server(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object, method, _filter_none(params)\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m     89\u001b[0m     {\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mon_error_future,\n\u001b[1;32m     91\u001b[0m         callback\u001b[38;5;241m.\u001b[39mfuture,\n\u001b[1;32m     92\u001b[0m     },\n\u001b[1;32m     93\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:426\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, loop, timeout, return_when)\u001b[0m\n\u001b[1;32m    420\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scheduled for removal in Python 3.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    424\u001b[0m fs \u001b[38;5;241m=\u001b[39m {ensure_future(f, loop\u001b[38;5;241m=\u001b[39mloop) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(fs)}\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:534\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    531\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import csv\n",
    "\n",
    "async def get_and_process_reviews(bike_name, page):\n",
    "    all_reviews = []\n",
    "    try:\n",
    "        page_number = 1\n",
    "        while True:  # Loop until no more reviews are found\n",
    "            try:\n",
    "                # Wait for the list of reviews to load and capture all li elements dynamically\n",
    "                await page.wait_for_selector(\"#user-review-header > div > div > div.o-brXWGL > div.o-dJmcbh > div > ul > li\", timeout=5000)\n",
    "                review_items = await page.query_selector_all(\"#user-review-header > div > div > div.o-brXWGL > div.o-dJmcbh > div > ul > li\")\n",
    "                print(f\"Found {len(review_items)} reviews on page {page_number}.\")\n",
    "\n",
    "                # Process each review item and extract all the required fields\n",
    "                for review_item in review_items:\n",
    "                    try:\n",
    "                        # Extract the title of the review\n",
    "                        title_element = await review_item.query_selector(\"div > div > a > p\")\n",
    "                        title_text = (await title_element.text_content()).strip() if title_element else \"No Title\"\n",
    "\n",
    "                        # Extract the review content\n",
    "                        review_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > p.o-eemiLE.o-cYdrZi.o-cJrNdO.o-fyWCgU\"\n",
    "                        )\n",
    "                        review_text = (await review_element.text_content()).strip() if review_element else \"No Review\"\n",
    "\n",
    "                        # Extract the time of the review\n",
    "                        time_element = await review_item.query_selector(\n",
    "                            \"div > div > div.o-fzpihx.o-fzptYC.o-eemiLE.o-zmksK.o-cpnuEd > p:nth-child(1)\"\n",
    "                        )\n",
    "                        time_text = (await time_element.text_content()).strip() if time_element else \"No Time\"\n",
    "\n",
    "                        # Extract \"Design and styling\"\n",
    "                        design_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div:nth-child(2) > div.o-biwSqu.o-qqdXv.o-XylGE.o-cpnuEd > div:nth-child(1) > div > div > span\"\n",
    "                        )\n",
    "                        design_text = (await design_element.text_content()).strip() if design_element else \"No Design Rating\"\n",
    "\n",
    "                        # Extract \"Reliability\"\n",
    "                        reliability_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div:nth-child(2) > div.o-biwSqu.o-qqdXv.o-XylGE.o-cpnuEd > div:nth-child(3) > div > div > span\"\n",
    "                        )\n",
    "                        reliability_text = (await reliability_element.text_content()).strip() if reliability_element else \"No Reliability Rating\"\n",
    "\n",
    "                        # Extract \"Comfort\"\n",
    "                        comfort_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div:nth-child(2) > div.o-biwSqu.o-qqdXv.o-XylGE.o-cpnuEd > div:nth-child(5) > div > div > span\"\n",
    "                        )\n",
    "                        comfort_text = (await comfort_element.text_content()).strip() if comfort_element else \"No Comfort Rating\"\n",
    "\n",
    "                        # Extract \"Service experience\"\n",
    "                        service_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div:nth-child(2) > div.o-biwSqu.o-qqdXv.o-XylGE.o-cpnuEd > div:nth-child(7) > div > div > span\"\n",
    "                        )\n",
    "                        service_text = (await service_element.text_content()).strip() if service_element else \"No Service Experience\"\n",
    "\n",
    "                        # Extract \"Value for money\"\n",
    "                        value_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div:nth-child(2) > div.o-biwSqu.o-qqdXv.o-XylGE.o-cpnuEd > div:nth-child(9) > div > div > span\"\n",
    "                        )\n",
    "                        value_text = (await value_element.text_content()).strip() if value_element else \"No Value Rating\"\n",
    "\n",
    "                        # Extract \"Used it for\"\n",
    "                        used_for_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div.eoWUQZ.o-bUVylL.o-XylGE.o-cpnuEd > div:nth-child(1) > p.o-eemiLE.o-eqqVmt.o-cJrNdO\"\n",
    "                        )\n",
    "                        used_for_text = (await used_for_element.text_content()).strip() if used_for_element else \"No Data\"\n",
    "\n",
    "                        # Extract \"Ridden for\"\n",
    "                        ridden_for_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div.eoWUQZ.o-bUVylL.o-XylGE.o-cpnuEd > div:nth-child(3) > p.o-eemiLE.o-eqqVmt.o-cJrNdO\"\n",
    "                        )\n",
    "                        ridden_for_text = (await ridden_for_element.text_content()).strip() if ridden_for_element else \"No Data\"\n",
    "\n",
    "                        # Extract \"Ridden for if owned\"\n",
    "                        ridden_owned_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div.eoWUQZ.o-bUVylL.o-XylGE.o-cpnuEd > div:nth-child(4) > p.o-eemiLE.o-eqqVmt.o-cJrNdO\"\n",
    "                        )\n",
    "                        ridden_owned_text = (await ridden_owned_element.text_content()).strip() if ridden_owned_element else \"No Data\"\n",
    "\n",
    "                        # Extract \"Owned for\"\n",
    "                        owned_for_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div.eoWUQZ.o-bUVylL.o-XylGE.o-cpnuEd > div:nth-child(2) > p.o-eemiLE.o-eqqVmt.o-cJrNdO\"\n",
    "                        )\n",
    "                        owned_for_text = (await owned_for_element.text_content()).strip() if owned_for_element else \"No Data\"\n",
    "\n",
    "                        # Extract \"Tips for other riders\"\n",
    "                        tips_element = await review_item.query_selector(\n",
    "                            \"div > div > div:nth-child(4) > div > div.o-bfyaNx.o-bNxxEB.o-bqHweY > div > div > div.o-fznVme > p.o-eemiLE.o-cJrNdO.o-fyWCgU.o-fzptUA\"\n",
    "                        )\n",
    "                        tips_text = (await tips_element.text_content()).strip() if tips_element else \"No Tips\"\n",
    "\n",
    "                        # Extract \"Review was useful\"\n",
    "                        useful_element = await review_item.query_selector(\n",
    "                            \"div > div > div.o-cpnuEd.o-dsiSgT.o-bUVylL.o-fznJzb > div > div:nth-child(1) > p\"\n",
    "                        )\n",
    "                        useful_text = (await useful_element.text_content()).strip() if useful_element else \"No Data\"\n",
    "\n",
    "                        # Extract \"Review was not useful\"\n",
    "                        not_useful_element = await review_item.query_selector(\n",
    "                            \"div > div > div.o-cpnuEd.o-dsiSgT.o-bUVylL.o-fznJzb > div > div:nth-child(3) > p\"\n",
    "                        )\n",
    "                        not_useful_text = (await not_useful_element.text_content()).strip() if not_useful_element else \"No Data\"\n",
    "\n",
    "                        # Extract number of filled stars by checking the class \"o-vYvcJ\"\n",
    "                        stars_container = await review_item.query_selector(\n",
    "                            \"div.o-fcaNGp.o-dsiSgT.o-NBTwp.o-dGBYL.o-bdcqQE\"\n",
    "                        )\n",
    "                        filled_star_elements = await stars_container.query_selector_all(\"svg.o-vYvcJ\")  # Count only filled stars\n",
    "                        num_stars = len(filled_star_elements) if filled_star_elements else 0  # Count number of filled stars\n",
    "\n",
    "                        # Append all the extracted fields to the all_reviews list\n",
    "                        all_reviews.append({\n",
    "                            'Bike': bike_name,\n",
    "                            'Stars': num_stars,  # Add the number of stars (integer value) at the beginning\n",
    "                            'Title': title_text,\n",
    "                            'Review': review_text,\n",
    "                            'Time': time_text,\n",
    "                            'Design and styling': design_text,\n",
    "                            'Reliability': reliability_text,\n",
    "                            'Comfort': comfort_text,\n",
    "                            'Service experience': service_text,\n",
    "                            'Value for money': value_text,\n",
    "                            'Used it for': used_for_text,\n",
    "                            'Ridden for': ridden_for_text,\n",
    "                            'Ridden for if owned': ridden_owned_text,\n",
    "                            'Owned for': owned_for_text,\n",
    "                            'Tips for other riders': tips_text,\n",
    "                            'Review was useful': useful_text,\n",
    "                            'Review was not useful': not_useful_text\n",
    "                        })\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing review: {e}\")\n",
    "\n",
    "                # After processing, check if there is a 'next' button to go to the next page\n",
    "                next_button = await page.query_selector(\".pagination-next\")\n",
    "                if next_button:\n",
    "                    await next_button.click()  # Click to go to the next page\n",
    "                    await page.wait_for_timeout(1000)  # Give some time for the next page to load\n",
    "                    page_number += 1  # Increment page number\n",
    "                else:\n",
    "                    break  # Exit the loop if no next button is found (end of reviews)\n",
    "\n",
    "            except TimeoutError:\n",
    "                print(\"Timeout while waiting for reviews to load.\")\n",
    "                break  # Exit if no reviews are loaded within the time limit\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in review processing: {e}\")\n",
    "    \n",
    "    return all_reviews\n",
    "\n",
    "\n",
    "# Function to save reviews to a CSV file with proper quoting\n",
    "def save_reviews_to_csv(reviews, file_name='reviews.csv'):\n",
    "    # Append to CSV after each page\n",
    "    with open(file_name, 'a', newline='', encoding='utf-8') as f_csv:\n",
    "        writer = csv.writer(f_csv, quoting=csv.QUOTE_MINIMAL)  # Ensure fields with commas are quoted\n",
    "        for review in reviews:\n",
    "            writer.writerow([\n",
    "                review.get('Bike', 'No Bike'),  # Stars column first\n",
    "                review.get('Stars', 'No Stars'),  # Stars column first\n",
    "                review.get('Title', 'No Title'),\n",
    "                review.get('Review', 'No Review'),\n",
    "                review.get('Time', 'No Time'),\n",
    "                review.get('Design and styling', 'No Design Rating'),\n",
    "                review.get('Reliability', 'No Reliability Rating'),\n",
    "                review.get('Comfort', 'No Comfort Rating'),\n",
    "                review.get('Service experience', 'No Service Experience'),\n",
    "                review.get('Value for money', 'No Value Rating'),\n",
    "                review.get('Used it for', 'No Data'),\n",
    "                review.get('Ridden for', 'No Data'),\n",
    "                review.get('Ridden for if owned', 'No Data'),\n",
    "                review.get('Owned for', 'No Data'),\n",
    "                review.get('Tips for other riders', 'No Tips'),\n",
    "                review.get('Review was useful', 'No Data'),\n",
    "                review.get('Review was not useful', 'No Data')\n",
    "            ])\n",
    "    print(f\"Saved {len(reviews)} reviews to {file_name}.\")\n",
    "\n",
    "# # Function to read URLs from a file\n",
    "# def read_urls_from_file(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         urls = [line.strip() for line in f.readlines()]\n",
    "#     return urls\n",
    "\n",
    "def read_urls_from_file(file_path):\n",
    "    bike_data = []\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as f_csv:\n",
    "        reader = csv.reader(f_csv)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            bike_data.append((row[0], row[1]))  # Extract the Text (bike name) and URL\n",
    "    return bike_data\n",
    "\n",
    "\n",
    "# Main function to scrape reviews asynchronously from multiple URLs\n",
    "async def scrape_reviews():\n",
    "    bike_data = read_urls_from_file('allUrls.txt')  # Read URLs from file\n",
    "    async with async_playwright() as playwright:\n",
    "        # Launch the Firefox browser in headless mode\n",
    "        browser = await playwright.firefox.launch(headless=True)\n",
    "        \n",
    "        file_name = 'reviews.csv'\n",
    "        # Write header to CSV\n",
    "        with open(file_name, 'w', newline='', encoding='utf-8') as f_csv:\n",
    "            writer = csv.writer(f_csv, quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([\n",
    "                'Bike', 'Stars', 'Title', 'Review', 'Time', 'Design and styling', 'Reliability', 'Comfort', \n",
    "                'Service experience', 'Value for money', 'Used it for', 'Ridden for',\n",
    "                'Ridden for if owned', 'Owned for', 'Tips for other riders', 'Review was useful', \n",
    "                'Review was not useful'\n",
    "            ])\n",
    "        \n",
    "        # Loop through each URL and scrape reviews\n",
    "        for bike_name, url in bike_data:\n",
    "            print(bike_name, url)\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url)\n",
    "\n",
    "            try:\n",
    "                while True:\n",
    "                    # Get and process reviews on the current page\n",
    "                    all_reviews = await get_and_process_reviews(bike_name, page)\n",
    "                    \n",
    "                    # Save the reviews to CSV after each page is processed\n",
    "                    if all_reviews:\n",
    "                        save_reviews_to_csv(all_reviews, file_name)\n",
    "\n",
    "                    # Check if the 'Next' button is available to go to the next page\n",
    "                    next_button = await page.query_selector(\"#user-review-header > div > ul > li.o-bdccbU.o-fzoTpF > a > span > svg\")\n",
    "                    if next_button:\n",
    "                        await next_button.click()\n",
    "                        await page.wait_for_timeout(2000)  # Wait for the next page to load\n",
    "                    else:\n",
    "                        print(f\"No more pages for {url}.\")\n",
    "                        break\n",
    "\n",
    "            except TimeoutError:\n",
    "                print(f\"Timeout while scraping {url}.\")\n",
    "            \n",
    "            # Close the page before moving to the next URL\n",
    "            await page.close()\n",
    "\n",
    "        # Close the browser once all URLs are processed\n",
    "        await browser.close()\n",
    "        print(\"Finished scraping all URLs.\")\n",
    "\n",
    "# Run the async function\n",
    "await scrape_reviews()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
