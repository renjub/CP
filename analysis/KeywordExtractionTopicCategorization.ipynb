{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/renju/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/renju/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/renju/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./extracted_bikewale_links_playwright_async.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The CSV file './extracted_bikewale_links_playwright_async.csv' must contain a 'Review' column.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Check if 'Review_Text' column exists\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe CSV file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must contain a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Apply the functions to extract keywords and assign buckets\u001b[39;00m\n\u001b[1;32m    102\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeywords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: extract_keywords_without_bike_name(x, bike_name_words, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    104\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: The CSV file './extracted_bikewale_links_playwright_async.csv' must contain a 'Review' column."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "tokenizers_list = ['punkt', 'wordnet', 'omw-1.4']\n",
    "for tokenizer in tokenizers_list:\n",
    "    nltk.download(tokenizer)\n",
    "\n",
    "# Load models\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')  # Improved keyword extraction\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')  # Most accurate semantic similarity model\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
    "\n",
    "# Synonym-enriched buckets with hierarchical sub-categories\n",
    "buckets = {\n",
    "    \"Mileage\": [\"Fuel Efficiency\", \"Fuel Economy\", \"Fuel Consumption\", \"Miles per Gallon\"],\n",
    "    \"Comfort\": [\"Riding Comfort\", \"Seat Comfort\", \"Suspension Comfort\", \"Vibration Control\"],\n",
    "    \"Looks\": [\"Design\", \"Exterior\", \"Aesthetics\", \"Styling\", \"Color Options\"],\n",
    "    \"Performance\": [\"Acceleration\", \"Handling\", \"Stability\", \"Braking Performance\", \"Cornering\"],\n",
    "    \"Power\": [\"Horsepower\", \"Torque\", \"Engine Output\", \"Power Delivery\"],\n",
    "    \"Engine\": [\"Engine Sound\", \"Engine Performance\", \"Engine Reliability\", \"Engine Smoothness\", \"Engine Cooling\"],\n",
    "    \"Experience\": [\"Riding Experience\", \"Ownership Experience\", \"Long Ride Experience\", \"Daily Ride Experience\"],\n",
    "    \"Speed\": [\"Top Speed\", \"Speed Pickup\", \"Speed Acceleration\", \"Speed Stability\"],\n",
    "    \"Price\": [\"Affordability\", \"Value for Money\", \"Cost Effectiveness\", \"Pricing Options\", \"Discounts\"],\n",
    "    \"Service\": [\"Service Cost\", \"Service Center Experience\", \"After Sales Service\", \"Service Quality\", \"Service Availability\"],\n",
    "    \"Pickup\": [\"Initial Pickup\", \"Throttle Response\", \"Low-End Torque\"],\n",
    "    \"Maintenance\": [\"Maintenance Cost\", \"Maintenance Frequency\", \"Maintenance Ease\", \"Spare Parts Availability\"],\n",
    "    \"Seat\": [\"Seat Comfort\", \"Seat Design\", \"Seat Material\", \"Seat Height\"]\n",
    "}\n",
    "\n",
    "# Apply lemmatization to normalize bucket names and sub-buckets\n",
    "buckets = {lemmatizer.lemmatize(bucket.lower()): [lemmatizer.lemmatize(sub.lower()) for sub in sub_buckets] for bucket, sub_buckets in buckets.items()}\n",
    "\n",
    "# Function to extract keywords for a single review\n",
    "def extract_keywords_without_bike_name(text, bike_name_words, top_n=3):\n",
    "    if pd.isna(text):  # Handle missing or NaN values\n",
    "        return []\n",
    "    \n",
    "    # Extract keywords using KeyBERT\n",
    "    keywords = kw_model.extract_keywords(text, \n",
    "                                         keyphrase_ngram_range=(1, 2), \n",
    "                                         stop_words=['english', 'bike', 'bajaj'] + bike_name_words, \n",
    "                                         top_n=top_n)\n",
    "    return [keyword for keyword, score in keywords]\n",
    "\n",
    "# Function to find the best matching bucket and sub-bucket\n",
    "def assign_bucket_and_sub_bucket(review_text, buckets, embedding_model, similarity_threshold=0.4):\n",
    "    if pd.isna(review_text):  # Handle missing or NaN values\n",
    "        return \"Other\", \"Other\"\n",
    "    \n",
    "    # Flatten buckets into a list of all buckets and sub-buckets\n",
    "    all_buckets = list(buckets.keys()) + [sub for sublist in buckets.values() for sub in sublist]\n",
    "\n",
    "    # Generate embeddings for buckets\n",
    "    bucket_embeddings = embedding_model.encode(all_buckets, convert_to_tensor=True)\n",
    "\n",
    "    # Generate embedding for the review text\n",
    "    review_embedding = embedding_model.encode(review_text, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarities = util.pytorch_cos_sim(review_embedding, bucket_embeddings)\n",
    "    max_similarity, best_bucket_index = torch.max(similarities, dim=1)\n",
    "\n",
    "    # Check if the similarity is above the threshold\n",
    "    best_bucket = all_buckets[best_bucket_index.item()]\n",
    "    if max_similarity.item() > similarity_threshold:\n",
    "        for bucket, sub_buckets in buckets.items():\n",
    "            if best_bucket == bucket or best_bucket in sub_buckets:\n",
    "                return bucket, best_bucket\n",
    "        return best_bucket, best_bucket\n",
    "    \n",
    "    # If no match found, assign \"Other\" bucket with the topmost keyword as sub-bucket\n",
    "    keywords = extract_keywords_without_bike_name(review_text, bike_name_words, top_n=1)\n",
    "    top_keyword = keywords[0] if keywords else \"Other\"\n",
    "    return \"Other\", top_keyword\n",
    "\n",
    "# Process all CSV files in the current working directory and subfolders\n",
    "for root, dirs, files in os.walk('.'):  # Traverse through all subfolders\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            # Extract bike name from file name\n",
    "            bike_name = \" \".join(file_path.split(\"_\")[:-1])  # Extract bike name from file name (without 'reviews')\n",
    "            bike_name_words = bike_name.lower().split()  # Split bike name into individual words\n",
    "            bike_name_words.extend(['bike', 'bajaj'])  # Add additional stop words\n",
    "\n",
    "            # Load the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Check if 'Review_Text' column exists\n",
    "            if 'Review' not in df.columns:\n",
    "                raise ValueError(f\"The CSV file '{file_path}' must contain a 'Review' column.\")\n",
    "\n",
    "            # Apply the functions to extract keywords and assign buckets\n",
    "            df['Keywords'] = df['Review'].apply(\n",
    "                lambda x: extract_keywords_without_bike_name(x, bike_name_words, top_n=3)\n",
    "            )\n",
    "            df[['Assigned_Bucket', 'Assigned_Sub_Bucket']] = df['Review'].apply(\n",
    "                lambda x: pd.Series(assign_bucket_and_sub_bucket(x, buckets, embedding_model, similarity_threshold=0.4))\n",
    "            )\n",
    "\n",
    "            # Convert keywords to a string for better readability in the output\n",
    "            df['Keywords'] = df['Keywords'].apply(lambda x: \", \".join(x) if x else \"\")\n",
    "\n",
    "            # Save the DataFrame to the original CSV file with new columns\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "            print(f\"Updated the input file '{file_path}' with keywords, assigned buckets, and assigned sub-buckets.\")\n",
    "            print(\"Keywords:\")\n",
    "            print(df['Keywords'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
