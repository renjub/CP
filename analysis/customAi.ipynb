{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 301.1163737233728\n",
      "Epoch 2 Loss: 278.5386562105268\n",
      "Epoch 3 Loss: 268.6856988798827\n",
      "Epoch 4 Loss: 257.68537754862336\n",
      "Epoch 5 Loss: 252.8323963517323\n",
      "Epoch 6 Loss: 241.64122542715631\n",
      "Epoch 7 Loss: 233.75642627407797\n",
      "Epoch 8 Loss: 223.726341615431\n",
      "Epoch 9 Loss: 217.585253006313\n",
      "Epoch 10 Loss: 209.12853974406607\n",
      "I absolutely love this! {'POSITIVE': 0.9106165766716003, 'NEUTRAL': 0.08803308010101318, 'NEGATIVE': 0.001350310631096363}\n",
      "This is terrible and I hate it. {'POSITIVE': 0.7780562043190002, 'NEUTRAL': 0.2100001871585846, 'NEGATIVE': 0.01194359827786684}\n",
      "I am not sure how I feel about this. {'POSITIVE': 0.2442249208688736, 'NEUTRAL': 0.30260375142097473, 'NEGATIVE': 0.4531712532043457}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load a blank SpaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Step 2: Add the text classification pipeline component to the model\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "# Step 3: Add labels for sentiment classification (positive, neutral, negative)\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEUTRAL\")\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "# Define a function to map star ratings to sentiment categories\n",
    "def map_rating_to_label(rating):\n",
    "    if rating >= 4:  # 4-5 stars as POSITIVE\n",
    "        return {\"POSITIVE\": 1, \"NEUTRAL\": 0, \"NEGATIVE\": 0}\n",
    "    elif rating == 3:  # 3 stars as NEUTRAL\n",
    "        return {\"POSITIVE\": 0, \"NEUTRAL\": 1, \"NEGATIVE\": 0}\n",
    "    else:  # 1-2 stars as NEGATIVE\n",
    "        return {\"POSITIVE\": 0, \"NEUTRAL\": 0, \"NEGATIVE\": 1}\n",
    "\n",
    "# Step 4: Load the list of file paths\n",
    "# Assuming you have a file `file_paths.txt` that contains paths to CSV files, one per line\n",
    "with open('csv_Files', 'r') as file:\n",
    "    file_paths = [line.strip() for line in file]\n",
    "\n",
    "# Initialize an empty list for storing training data\n",
    "train_data = []\n",
    "\n",
    "# Loop through each file path and process the data\n",
    "for file_path in file_paths:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Clean and process the data\n",
    "    df = df.dropna(subset=['Review Text', 'Rating'])  # Drop rows with missing review text or rating\n",
    "    df['Review Text'] = df['Review Text'].astype(str)  # Ensure all review texts are strings\n",
    "\n",
    "    # Iterate through each row in the DataFrame and process the review text and rating\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['Review Text']\n",
    "        rating = row['Rating']\n",
    "        cats = map_rating_to_label(rating)\n",
    "        train_data.append((text, {\"cats\": cats}))\n",
    "\n",
    "# Step 5: Train the model with the combined data\n",
    "# Disable other pipeline components to only train the textcat component\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # Only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):  # Train for 10 epochs\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        # Use minibatch training\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                examples.append(Example.from_dict(doc, annotations))\n",
    "            nlp.update(examples, drop=0.2, losses=losses)\n",
    "        print(f\"Epoch {epoch + 1} Loss: {losses['textcat']}\")\n",
    "\n",
    "# Step 6: Test the model with new examples\n",
    "test_texts = [\n",
    "    \"I absolutely love this!\",\n",
    "    \"This is terrible and I hate it.\",\n",
    "    \"I am not sure how I feel about this.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    doc = nlp(text)\n",
    "    print(text, doc.cats)  # Returns a dictionary with POSITIVE/NEUTRAL/NEGATIVE scores\n",
    "\n",
    "# Step 7: Save the trained model to a directory\n",
    "nlp.to_disk(\"sentiment_model\")\n",
    "\n",
    "# To load the model later for use\n",
    "# nlp2 = spacy.load(\"sentiment_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
