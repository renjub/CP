{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "BrowserType.launch: Executable doesn't exist at /home/renju/.cache/ms-playwright/chromium-1134/chrome-linux/chrome\n╔════════════════════════════════════════════════════════════╗\n║ Looks like Playwright was just installed or updated.       ║\n║ Please run the following command to download new browsers: ║\n║                                                            ║\n║     playwright install                                     ║\n║                                                            ║\n║ <3 Playwright Team                                         ║\n╚════════════════════════════════════════════════════════════╝",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews extraction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# For environments with a running event loop\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_scraper()\n",
      "Cell \u001b[0;32mIn[1], line 190\u001b[0m, in \u001b[0;36mrun_scraper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_scraper\u001b[39m():\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 190\u001b[0m         browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    191\u001b[0m         page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;66;03m# Load the CSV file with the links\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/async_api/_generated.py:14115\u001b[0m, in \u001b[0;36mBrowserType.launch\u001b[0;34m(self, executable_path, channel, args, ignore_default_args, handle_sigint, handle_sigterm, handle_sighup, timeout, env, headless, devtools, proxy, downloads_path, slow_mo, traces_dir, chromium_sandbox, firefox_user_prefs)\u001b[0m\n\u001b[1;32m  14000\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlaunch\u001b[39m(\n\u001b[1;32m  14001\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  14002\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  14023\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m  14024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrowser\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  14025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BrowserType.launch\u001b[39;00m\n\u001b[1;32m  14026\u001b[0m \n\u001b[1;32m  14027\u001b[0m \u001b[38;5;124;03m    Returns the browser instance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  14111\u001b[0m \u001b[38;5;124;03m    Browser\u001b[39;00m\n\u001b[1;32m  14112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m  14114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl(\n\u001b[0;32m> 14115\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mlaunch(\n\u001b[1;32m  14116\u001b[0m             executablePath\u001b[38;5;241m=\u001b[39mexecutable_path,\n\u001b[1;32m  14117\u001b[0m             channel\u001b[38;5;241m=\u001b[39mchannel,\n\u001b[1;32m  14118\u001b[0m             args\u001b[38;5;241m=\u001b[39mmapping\u001b[38;5;241m.\u001b[39mto_impl(args),\n\u001b[1;32m  14119\u001b[0m             ignoreDefaultArgs\u001b[38;5;241m=\u001b[39mmapping\u001b[38;5;241m.\u001b[39mto_impl(ignore_default_args),\n\u001b[1;32m  14120\u001b[0m             handleSIGINT\u001b[38;5;241m=\u001b[39mhandle_sigint,\n\u001b[1;32m  14121\u001b[0m             handleSIGTERM\u001b[38;5;241m=\u001b[39mhandle_sigterm,\n\u001b[1;32m  14122\u001b[0m             handleSIGHUP\u001b[38;5;241m=\u001b[39mhandle_sighup,\n\u001b[1;32m  14123\u001b[0m             timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m  14124\u001b[0m             env\u001b[38;5;241m=\u001b[39mmapping\u001b[38;5;241m.\u001b[39mto_impl(env),\n\u001b[1;32m  14125\u001b[0m             headless\u001b[38;5;241m=\u001b[39mheadless,\n\u001b[1;32m  14126\u001b[0m             devtools\u001b[38;5;241m=\u001b[39mdevtools,\n\u001b[1;32m  14127\u001b[0m             proxy\u001b[38;5;241m=\u001b[39mproxy,\n\u001b[1;32m  14128\u001b[0m             downloadsPath\u001b[38;5;241m=\u001b[39mdownloads_path,\n\u001b[1;32m  14129\u001b[0m             slowMo\u001b[38;5;241m=\u001b[39mslow_mo,\n\u001b[1;32m  14130\u001b[0m             tracesDir\u001b[38;5;241m=\u001b[39mtraces_dir,\n\u001b[1;32m  14131\u001b[0m             chromiumSandbox\u001b[38;5;241m=\u001b[39mchromium_sandbox,\n\u001b[1;32m  14132\u001b[0m             firefoxUserPrefs\u001b[38;5;241m=\u001b[39mmapping\u001b[38;5;241m.\u001b[39mto_impl(firefox_user_prefs),\n\u001b[1;32m  14133\u001b[0m         )\n\u001b[1;32m  14134\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_browser_type.py:95\u001b[0m, in \u001b[0;36mBrowserType.launch\u001b[0;34m(self, executablePath, channel, args, ignoreDefaultArgs, handleSIGINT, handleSIGTERM, handleSIGHUP, timeout, env, headless, devtools, proxy, downloadsPath, slowMo, tracesDir, chromiumSandbox, firefoxUserPrefs)\u001b[0m\n\u001b[1;32m     92\u001b[0m params \u001b[38;5;241m=\u001b[39m locals_to_params(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m     93\u001b[0m normalize_launch_params(params)\n\u001b[1;32m     94\u001b[0m browser \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m---> 95\u001b[0m     Browser, from_channel(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaunch\u001b[39m\u001b[38;5;124m\"\u001b[39m, params))\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_did_launch_browser(browser)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m browser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:59\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:514\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mError\u001b[0m: BrowserType.launch: Executable doesn't exist at /home/renju/.cache/ms-playwright/chromium-1134/chrome-linux/chrome\n╔════════════════════════════════════════════════════════════╗\n║ Looks like Playwright was just installed or updated.       ║\n║ Please run the following command to download new browsers: ║\n║                                                            ║\n║     playwright install                                     ║\n║                                                            ║\n║ <3 Playwright Team                                         ║\n╚════════════════════════════════════════════════════════════╝"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract star rating from the review_div\n",
    "def extract_star_rating_from_review(review_div):\n",
    "    rating_div = review_div.find('div', class_='rating')\n",
    "    if rating_div:\n",
    "        star_span = rating_div.find('span', recursive=False)\n",
    "        if star_span:\n",
    "            rated_stars = star_span.find_all('i', class_='icon-rating rated-star')\n",
    "            star_rating = len(rated_stars)\n",
    "        else:\n",
    "            star_rating = 0\n",
    "    else:\n",
    "        star_rating = 0\n",
    "    return star_rating\n",
    "\n",
    "# Function to extract review title from the review_div\n",
    "def extract_review_title(review_div):\n",
    "    # Use the appropriate pattern to find the title element\n",
    "    title_element = review_div.find('a', id=re.compile(r'rptreviews_ctl\\d+_lnkTitle'))\n",
    "    \n",
    "    if title_element:\n",
    "        review_title = title_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_title = \"-\"\n",
    "    \n",
    "    return review_title\n",
    "\n",
    "# Function to extract review text from the review_div\n",
    "def extract_review_text(review_div):\n",
    "    review_element = review_div.select_one(\"div.more.reviewdata > p\")\n",
    "    if review_element:\n",
    "        review_text = review_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_text = \"-\"\n",
    "    return review_text\n",
    "\n",
    "# Function to extract date and time from the review_div\n",
    "def extract_review_datetime(review_div):\n",
    "    datetime_element = review_div.find('span', id=re.compile(r'^rptreviews_ctl\\d+_lblDateTime$'))\n",
    "    if datetime_element:\n",
    "        review_datetime = datetime_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_datetime = \"-\"\n",
    "    return review_datetime\n",
    "\n",
    "# Function to extract location from the review_div\n",
    "def extract_location(review_div):\n",
    "    # Use a regular expression to match the div with id containing 'rptreviews_ctl<number>_divProfile'\n",
    "    location_div = review_div.find(\"div\", id=re.compile(r'rptreviews_ctl\\d+_divProfile'))\n",
    "    \n",
    "    if location_div:\n",
    "        # Now find the div containing the actual address text\n",
    "        address_div = location_div.find(\"div\", class_=\"usr-addr-text\")\n",
    "        if address_div:\n",
    "            return address_div.get_text(strip=True)\n",
    "    \n",
    "    return \"-\"    \n",
    "\n",
    "# Function to extract likes from the review_div\n",
    "def extract_likes(review_div):\n",
    "    likes_element = review_div.select_one(\"[id^=rptreviews_ctl][id$=_divlike] > a\")\n",
    "    if likes_element:\n",
    "        likes_text = likes_element.get_text(strip=True)\n",
    "        likes = re.search(r'\\d+', likes_text)\n",
    "        likes = likes.group() if likes else \"0\"\n",
    "    else:\n",
    "        likes = \"0\"\n",
    "    return int(likes)\n",
    "\n",
    "\n",
    "# Function to extract comments from the review_div\n",
    "def extract_num_comments(review_div):\n",
    "    comments_element = review_div.select_one(\"[id^=rptreviews_ctl][id$=_commentspan]\")\n",
    "    if comments_element:\n",
    "        comments_text = comments_element.get_text(strip=True)\n",
    "        comments = re.search(r'\\d+', comments_text)\n",
    "        comments = comments.group() if comments else \"0\"\n",
    "    else:\n",
    "        comments = \"0\"\n",
    "    return int(comments)\n",
    "\n",
    "def extract_fake_status(review_div):\n",
    "    # Adjusted selector to handle dynamic number in the pattern\n",
    "    fake_element = review_div.select_one(\"div.stamp > div > span[id^='rptreviews_ctl'][id$='_commentspan']\")\n",
    "    if fake_element:\n",
    "        fake_status = fake_element.get_text(strip=True)\n",
    "    else:\n",
    "        fake_status = \"0\"\n",
    "    return fake_status\n",
    "\n",
    "# Function to read the last page number\n",
    "async def get_last_page_number(page):\n",
    "    try:\n",
    "        last_page_selector = \"#spnPaging > li:nth-child(12) > a\"\n",
    "        last_page_element = await page.query_selector(last_page_selector)\n",
    "        if last_page_element:\n",
    "            last_page_text = await last_page_element.inner_text()\n",
    "            return int(last_page_text.strip())\n",
    "        else:\n",
    "            return 1  # Fallback to 1 if last page is not found\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding last page number: {e}\")\n",
    "        return 1\n",
    "\n",
    "async def extract_reviews(page, base_url, last_page, product_name):\n",
    "    review_count = 0\n",
    "\n",
    "    # Create a folder for the product if it doesn't exist\n",
    "    folder_name = product_name.replace(\" \", \"_\")  # Use underscores instead of spaces for folder names\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for page_number in range(1, last_page + 1):\n",
    "        try:\n",
    "            # Construct the URL for the current page\n",
    "            current_url = f\"{base_url}-page-{page_number}\"\n",
    "            print(f\"Scraping page {page_number} from URL: {current_url}...\")\n",
    "\n",
    "            await page.goto(current_url)\n",
    "\n",
    "            # Add a delay to ensure the page fully loads\n",
    "            await page.wait_for_timeout(2000)\n",
    "\n",
    "            page_source = await page.content()\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Use a selector to find div elements with class \"row review-article\"\n",
    "            review_divs = soup.select('#dvreview-listing > div[class=\"row review-article\"]')\n",
    "\n",
    "            all_reviews = []\n",
    "\n",
    "            # Extract data from each review\n",
    "            for review_div in review_divs:\n",
    "                review_title    = extract_review_title(review_div)  # Extract the review title\n",
    "                review_text     = extract_review_text(review_div)\n",
    "                star_rating     = extract_star_rating_from_review(review_div)\n",
    "                review_datetime = extract_review_datetime(review_div)\n",
    "                location        = extract_location(review_div)\n",
    "                likes           = extract_likes(review_div)\n",
    "                comments        = extract_num_comments(review_div)\n",
    "                fake_status     = extract_fake_status(review_div)\n",
    "\n",
    "                review_data = {\n",
    "                    'Product'      : product_name,\n",
    "                    'Title'        : review_title,  # Add review title to the data\n",
    "                    'Rating'       : star_rating,\n",
    "                    'Review Text'  : review_text,\n",
    "                    'Date and Time': review_datetime,\n",
    "                    'Location'     : location,\n",
    "                    'Likes'        : likes,\n",
    "                    'Comments'     : comments,\n",
    "                    'Fake Status'  : fake_status,\n",
    "                }\n",
    "\n",
    "                all_reviews.append(review_data)\n",
    "\n",
    "            # Save reviews to a CSV file per page in the product folder\n",
    "            if all_reviews:\n",
    "                df = pd.DataFrame(all_reviews)\n",
    "                # Reorder columns to include 'Title' and move 'Product' to the beginning\n",
    "                columns_order = ['Product', 'Rating', 'Title', 'Review Text', 'Date and Time', 'Location', 'Likes', 'Comments', 'Fake Status']\n",
    "                df = df[columns_order]\n",
    "                \n",
    "                # Save the DataFrame to CSV\n",
    "                csv_filename = os.path.join(folder_name, f\"{product_name.replace(' ', '_')}_page_{page_number}.csv\")\n",
    "                df.to_csv(csv_filename, mode='w', header=True, index=False)\n",
    "\n",
    "            review_count += len(review_divs)\n",
    "\n",
    "            if page_number == last_page:\n",
    "                print(f\"Reached the last page: {last_page}. Exiting.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during review extraction: {e}\")\n",
    "            break\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Updated run_scraper to pass the current URL\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Load the CSV file with the links\n",
    "        file_path = 'extracted_links_temp.csv'\n",
    "        extracted_links_df = pd.read_csv(file_path)\n",
    "        print(extracted_links_df)\n",
    "\n",
    "        for index, row in extracted_links_df.iterrows():\n",
    "            url = row['Link']\n",
    "            await page.goto(url)\n",
    "            await page.wait_for_timeout(2000)  # Wait for 2 seconds\n",
    "\n",
    "            # Get the last page number\n",
    "            last_page = await get_last_page_number(page)\n",
    "            print(f\"Last page number: {last_page}\")\n",
    "\n",
    "            # Extract reviews until the last page\n",
    "            product_name = row['Text']\n",
    "            await extract_reviews(page, url, last_page, product_name)\n",
    "\n",
    "        await browser.close()\n",
    "        print(\"Reviews extraction complete.\")\n",
    "\n",
    "# For environments with a running event loop\n",
    "await run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
