{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Text                                               Link\n",
      "0  bajaj_avenger  https://www.mouthshut.com/bikes/bajaj-avenger-...\n",
      "Last page number: 148\n",
      "Scraping page 1 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-925049340-page-1...\n",
      "Scraping page 2 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-925049340-page-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=TargetClosedError('Target page, context or browser has been closed')>\n",
      "playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 3 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-925049340-page-3...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 215\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews extraction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# For environments with a running event loop\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_scraper()\n",
      "Cell \u001b[0;32mIn[3], line 209\u001b[0m, in \u001b[0;36mrun_scraper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Extract reviews until the last page\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     product_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m extract_reviews(page, url, last_page, product_name)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews extraction complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 126\u001b[0m, in \u001b[0;36mextract_reviews\u001b[0;34m(page, base_url, last_page, product_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m current_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-page-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgoto(current_url)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Add a delay to ensure the page fully loads\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mwait_for_timeout(\u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/async_api/_generated.py:8810\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, wait_until, referer)\u001b[0m\n\u001b[1;32m   8749\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m   8750\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8751\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8757\u001b[0m     referer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   8758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   8759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.goto\u001b[39;00m\n\u001b[1;32m   8760\u001b[0m \n\u001b[1;32m   8761\u001b[0m \u001b[38;5;124;03m    Returns the main resource response. In case of multiple redirects, the navigation will resolve with the first\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8806\u001b[0m \u001b[38;5;124;03m    Union[Response, None]\u001b[39;00m\n\u001b[1;32m   8807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   8809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl_nullable(\n\u001b[0;32m-> 8810\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mgoto(\n\u001b[1;32m   8811\u001b[0m             url\u001b[38;5;241m=\u001b[39murl, timeout\u001b[38;5;241m=\u001b[39mtimeout, waitUntil\u001b[38;5;241m=\u001b[39mwait_until, referer\u001b[38;5;241m=\u001b[39mreferer\n\u001b[1;32m   8812\u001b[0m         )\n\u001b[1;32m   8813\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_page.py:524\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    523\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mgoto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_frame.py:145\u001b[0m, in \u001b[0;36mFrame.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    143\u001b[0m         Optional[Response],\n\u001b[1;32m    144\u001b[0m         from_nullable_channel(\n\u001b[0;32m--> 145\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoto\u001b[39m\u001b[38;5;124m\"\u001b[39m, locals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m    146\u001b[0m         ),\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:59\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:512\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(parsed_st)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:88\u001b[0m, in \u001b[0;36mChannel.inner_send\u001b[0;34m(self, method, params, return_as_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m     85\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_send_message_to_server(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object, method, _filter_none(params)\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m     89\u001b[0m     {\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mon_error_future,\n\u001b[1;32m     91\u001b[0m         callback\u001b[38;5;241m.\u001b[39mfuture,\n\u001b[1;32m     92\u001b[0m     },\n\u001b[1;32m     93\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:426\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, loop, timeout, return_when)\u001b[0m\n\u001b[1;32m    420\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scheduled for removal in Python 3.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    424\u001b[0m fs \u001b[38;5;241m=\u001b[39m {ensure_future(f, loop\u001b[38;5;241m=\u001b[39mloop) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(fs)}\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:534\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    531\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract star rating from the review_div\n",
    "def extract_star_rating_from_review(review_div):\n",
    "    rating_div = review_div.find('div', class_='rating')\n",
    "    if rating_div:\n",
    "        star_span = rating_div.find('span', recursive=False)\n",
    "        if star_span:\n",
    "            rated_stars = star_span.find_all('i', class_='icon-rating rated-star')\n",
    "            star_rating = len(rated_stars)\n",
    "        else:\n",
    "            star_rating = 0\n",
    "    else:\n",
    "        star_rating = 0\n",
    "    return star_rating\n",
    "\n",
    "# Function to extract review title from the review_div\n",
    "def extract_review_title(review_div):\n",
    "    # Use the appropriate pattern to find the title element\n",
    "    title_element = review_div.find('a', id=re.compile(r'rptreviews_ctl\\d+_lnkTitle'))\n",
    "    \n",
    "    if title_element:\n",
    "        review_title = title_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_title = \"-\"\n",
    "    \n",
    "    return review_title\n",
    "\n",
    "# Function to extract review text from the review_div\n",
    "def extract_review_text(review_div):\n",
    "    review_element = review_div.select_one(\"div.more.reviewdata > p\")\n",
    "    if review_element:\n",
    "        review_text = review_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_text = \"-\"\n",
    "    return review_text\n",
    "\n",
    "# Function to extract date and time from the review_div\n",
    "def extract_review_datetime(review_div):\n",
    "    datetime_element = review_div.find('span', id=re.compile(r'^rptreviews_ctl\\d+_lblDateTime$'))\n",
    "    if datetime_element:\n",
    "        review_datetime = datetime_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_datetime = \"-\"\n",
    "    return review_datetime\n",
    "\n",
    "# Function to extract location from the review_div\n",
    "def extract_location(review_div):\n",
    "    # Use a regular expression to match the div with id containing 'rptreviews_ctl<number>_divProfile'\n",
    "    location_div = review_div.find(\"div\", id=re.compile(r'rptreviews_ctl\\d+_divProfile'))\n",
    "    \n",
    "    if location_div:\n",
    "        # Now find the div containing the actual address text\n",
    "        address_div = location_div.find(\"div\", class_=\"usr-addr-text\")\n",
    "        if address_div:\n",
    "            return address_div.get_text(strip=True)\n",
    "    \n",
    "    return \"-\"    \n",
    "\n",
    "# Function to extract likes from the review_div\n",
    "def extract_likes(review_div):\n",
    "    likes_element = review_div.select_one(\"[id^=rptreviews_ctl][id$=_divlike] > a\")\n",
    "    if likes_element:\n",
    "        likes_text = likes_element.get_text(strip=True)\n",
    "        likes = re.search(r'\\d+', likes_text)\n",
    "        likes = likes.group() if likes else \"0\"\n",
    "    else:\n",
    "        likes = \"0\"\n",
    "    return int(likes)\n",
    "\n",
    "\n",
    "# Function to extract comments from the review_div\n",
    "def extract_num_comments(review_div):\n",
    "    comments_element = review_div.select_one(\"[id^=rptreviews_ctl][id$=_commentspan]\")\n",
    "    if comments_element:\n",
    "        comments_text = comments_element.get_text(strip=True)\n",
    "        comments = re.search(r'\\d+', comments_text)\n",
    "        comments = comments.group() if comments else \"0\"\n",
    "    else:\n",
    "        comments = \"0\"\n",
    "    return int(comments)\n",
    "\n",
    "def extract_fake_status(review_div):\n",
    "    # Adjusted selector to handle dynamic number in the pattern\n",
    "    fake_element = review_div.select_one(\"div.stamp > div > span[id^='rptreviews_ctl'][id$='_commentspan']\")\n",
    "    if fake_element:\n",
    "        fake_status = fake_element.get_text(strip=True)\n",
    "    else:\n",
    "        fake_status = \"0\"\n",
    "    return fake_status\n",
    "\n",
    "# Function to read the last page number\n",
    "async def get_last_page_number(page):\n",
    "    try:\n",
    "        last_page_selector = \"#spnPaging > li:nth-child(12) > a\"\n",
    "        last_page_element = await page.query_selector(last_page_selector)\n",
    "        if last_page_element:\n",
    "            last_page_text = await last_page_element.inner_text()\n",
    "            return int(last_page_text.strip())\n",
    "        else:\n",
    "            return 1  # Fallback to 1 if last page is not found\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding last page number: {e}\")\n",
    "        return 1\n",
    "\n",
    "async def extract_reviews(page, base_url, last_page, product_name):\n",
    "    review_count = 0\n",
    "\n",
    "    # Create a folder for the product if it doesn't exist\n",
    "    folder_name = product_name.replace(\" \", \"_\")  # Use underscores instead of spaces for folder names\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for page_number in range(1, last_page + 1):\n",
    "        try:\n",
    "            # Construct the URL for the current page\n",
    "            current_url = f\"{base_url}-page-{page_number}\"\n",
    "            print(f\"Scraping page {page_number} from URL: {current_url}...\")\n",
    "\n",
    "            await page.goto(current_url)\n",
    "\n",
    "            # Add a delay to ensure the page fully loads\n",
    "            await page.wait_for_timeout(2000)\n",
    "\n",
    "            page_source = await page.content()\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Use a selector to find div elements with class \"row review-article\"\n",
    "            review_divs = soup.select('#dvreview-listing > div[class=\"row review-article\"]')\n",
    "\n",
    "            all_reviews = []\n",
    "\n",
    "            # Extract data from each review\n",
    "            for review_div in review_divs:\n",
    "                review_title    = extract_review_title(review_div)  # Extract the review title\n",
    "                review_text     = extract_review_text(review_div)\n",
    "                star_rating     = extract_star_rating_from_review(review_div)\n",
    "                review_datetime = extract_review_datetime(review_div)\n",
    "                location        = extract_location(review_div)\n",
    "                likes           = extract_likes(review_div)\n",
    "                comments        = extract_num_comments(review_div)\n",
    "                fake_status     = extract_fake_status(review_div)\n",
    "\n",
    "                review_data = {\n",
    "                    'Product'      : product_name,\n",
    "                    'Title'        : review_title,  # Add review title to the data\n",
    "                    'Rating'       : star_rating,\n",
    "                    'Review Text'  : review_text,\n",
    "                    'Date and Time': review_datetime,\n",
    "                    'Location'     : location,\n",
    "                    'Likes'        : likes,\n",
    "                    'Comments'     : comments,\n",
    "                    'Fake Status'  : fake_status,\n",
    "                }\n",
    "\n",
    "                all_reviews.append(review_data)\n",
    "\n",
    "            # Save reviews to a CSV file per page in the product folder\n",
    "            if all_reviews:\n",
    "                df = pd.DataFrame(all_reviews)\n",
    "                # Reorder columns to include 'Title' and move 'Product' to the beginning\n",
    "                columns_order = ['Product', 'Rating', 'Title', 'Review Text', 'Date and Time', 'Location', 'Likes', 'Comments', 'Fake Status']\n",
    "                df = df[columns_order]\n",
    "                \n",
    "                # Save the DataFrame to CSV\n",
    "                csv_filename = os.path.join(folder_name, f\"{product_name.replace(' ', '_')}_page_{page_number}.csv\")\n",
    "                df.to_csv(csv_filename, mode='w', header=True, index=False)\n",
    "\n",
    "            review_count += len(review_divs)\n",
    "\n",
    "            if page_number == last_page:\n",
    "                print(f\"Reached the last page: {last_page}. Exiting.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during review extraction: {e}\")\n",
    "            break\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Updated run_scraper to pass the current URL\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Load the CSV file with the links\n",
    "        file_path = 'extracted_links.csv'\n",
    "        extracted_links_df = pd.read_csv(file_path)\n",
    "        print(extracted_links_df)\n",
    "\n",
    "        for index, row in extracted_links_df.iterrows():\n",
    "            url = row['Link']\n",
    "            await page.goto(url)\n",
    "            await page.wait_for_timeout(2000)  # Wait for 2 seconds\n",
    "\n",
    "            # Get the last page number\n",
    "            last_page = await get_last_page_number(page)\n",
    "            print(f\"Last page number: {last_page}\")\n",
    "\n",
    "            # Extract reviews until the last page\n",
    "            product_name = row['Text']\n",
    "            await extract_reviews(page, url, last_page, product_name)\n",
    "\n",
    "        await browser.close()\n",
    "        print(\"Reviews extraction complete.\")\n",
    "\n",
    "# For environments with a running event loop\n",
    "await run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
