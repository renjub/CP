{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Text                                               Link\n",
      "0  bajaj_avenger  https://www.mouthshut.com/bikes/bajaj-avenger-...\n",
      "Last page number: 148\n",
      "Scraping page 1 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-1...\n",
      "Scraping page 2 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-2...\n",
      "Scraping page 3 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-3...\n",
      "Scraping page 4 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-4...\n",
      "Scraping page 5 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-5...\n",
      "Scraping page 6 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-6...\n",
      "Scraping page 7 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-7...\n",
      "Scraping page 8 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-8...\n",
      "Scraping page 9 from URL: https://www.mouthshut.com/bikes/bajaj-avenger-reviews-page-9...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews extraction complete. Data saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_reviews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# For Jupyter or environments with a running event loop\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_scraper()\n",
      "Cell \u001b[0;32mIn[2], line 174\u001b[0m, in \u001b[0;36mrun_scraper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m base_url \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Extract reviews until the last page\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_reviews(page, base_url, last_page)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Extract the website from the URL\u001b[39;00m\n\u001b[1;32m    177\u001b[0m website \u001b[38;5;241m=\u001b[39m extract_website(url)\n",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m, in \u001b[0;36mextract_reviews\u001b[0;34m(page, base_url, last_page)\u001b[0m\n\u001b[1;32m    107\u001b[0m current_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-page-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgoto(current_url)\n\u001b[1;32m    111\u001b[0m page_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mcontent()\n\u001b[1;32m    112\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/async_api/_generated.py:8810\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, wait_until, referer)\u001b[0m\n\u001b[1;32m   8749\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m   8750\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8751\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8757\u001b[0m     referer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   8758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   8759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.goto\u001b[39;00m\n\u001b[1;32m   8760\u001b[0m \n\u001b[1;32m   8761\u001b[0m \u001b[38;5;124;03m    Returns the main resource response. In case of multiple redirects, the navigation will resolve with the first\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8806\u001b[0m \u001b[38;5;124;03m    Union[Response, None]\u001b[39;00m\n\u001b[1;32m   8807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   8809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_impl_nullable(\n\u001b[0;32m-> 8810\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39mgoto(\n\u001b[1;32m   8811\u001b[0m             url\u001b[38;5;241m=\u001b[39murl, timeout\u001b[38;5;241m=\u001b[39mtimeout, waitUntil\u001b[38;5;241m=\u001b[39mwait_until, referer\u001b[38;5;241m=\u001b[39mreferer\n\u001b[1;32m   8812\u001b[0m         )\n\u001b[1;32m   8813\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_page.py:524\u001b[0m, in \u001b[0;36mPage.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    523\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39mgoto(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_frame.py:145\u001b[0m, in \u001b[0;36mFrame.goto\u001b[0;34m(self, url, timeout, waitUntil, referer)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoto\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     referer: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Response]:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    143\u001b[0m         Optional[Response],\n\u001b[1;32m    144\u001b[0m         from_nullable_channel(\n\u001b[0;32m--> 145\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoto\u001b[39m\u001b[38;5;124m\"\u001b[39m, locals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n\u001b[1;32m    146\u001b[0m         ),\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:59\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:512\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(parsed_st)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/playwright/_impl/_connection.py:88\u001b[0m, in \u001b[0;36mChannel.inner_send\u001b[0;34m(self, method, params, return_as_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m     85\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_send_message_to_server(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object, method, _filter_none(params)\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m     89\u001b[0m     {\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mon_error_future,\n\u001b[1;32m     91\u001b[0m         callback\u001b[38;5;241m.\u001b[39mfuture,\n\u001b[1;32m     92\u001b[0m     },\n\u001b[1;32m     93\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_COMPLETED,\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     callback\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:426\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, loop, timeout, return_when)\u001b[0m\n\u001b[1;32m    420\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand scheduled for removal in Python 3.10.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    424\u001b[0m fs \u001b[38;5;241m=\u001b[39m {ensure_future(f, loop\u001b[38;5;241m=\u001b[39mloop) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(fs)}\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:534\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    531\u001b[0m     f\u001b[38;5;241m.\u001b[39madd_done_callback(_on_completion)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract star rating from the review_div\n",
    "def extract_star_rating_from_review(review_div):\n",
    "    rating_div = review_div.find('div', class_='rating')\n",
    "    if rating_div:\n",
    "        star_span = rating_div.find('span', recursive=False)\n",
    "        if star_span:\n",
    "            rated_stars = star_span.find_all('i', class_='icon-rating rated-star')\n",
    "            star_rating = len(rated_stars)\n",
    "        else:\n",
    "            star_rating = 0\n",
    "    else:\n",
    "        star_rating = 0\n",
    "    return star_rating\n",
    "\n",
    "# Function to extract review text from the review_div\n",
    "def extract_review_text(review_div):\n",
    "    review_element = review_div.select_one(\"div.more.reviewdata > p\")\n",
    "    if review_element:\n",
    "        review_text = review_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_text = \"No review text available.\"\n",
    "    return review_text\n",
    "\n",
    "# Function to extract date and time from the review_div\n",
    "def extract_review_datetime(review_div):\n",
    "    datetime_element = review_div.find('span', id=re.compile(r'^rptreviews_ctl\\d+_lblDateTime$'))\n",
    "    if datetime_element:\n",
    "        review_datetime = datetime_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_datetime = \"No date and time available.\"\n",
    "    return review_datetime\n",
    "\n",
    "# Function to extract location from the review_div\n",
    "def extract_location(review_div):\n",
    "    location_element = review_div.select_one(\"#rptreviews_ctl00_divProfile > div.usr-addr-text\")\n",
    "    if location_element:\n",
    "        location = location_element.get_text(strip=True)\n",
    "    else:\n",
    "        location = \"No location available.\"\n",
    "    return location\n",
    "\n",
    "# Function to extract likes from the review_div\n",
    "def extract_likes(review_div):\n",
    "    likes_element = review_div.select_one(\"#rptreviews_ctl00_divlike > a\")\n",
    "    if likes_element:\n",
    "        likes_text = likes_element.get_text(strip=True)\n",
    "        likes = re.search(r'\\d+', likes_text)\n",
    "        likes = likes.group() if likes else \"0\"\n",
    "    else:\n",
    "        likes = \"0\"\n",
    "    return int(likes)\n",
    "\n",
    "# Function to extract comments from the review_div\n",
    "def extract_comments(review_div):\n",
    "    comments_element = review_div.select_one(\"#rptreviews_ctl02_commentspan\")\n",
    "    if comments_element:\n",
    "        comments_text = comments_element.get_text(strip=True)\n",
    "        comments = re.search(r'\\d+', comments_text)\n",
    "        comments = comments.group() if comments else \"0\"\n",
    "    else:\n",
    "        comments = \"0\"\n",
    "    return int(comments)\n",
    "\n",
    "# Function to extract fake status from the review_div\n",
    "def extract_fake_status(review_div):\n",
    "    fake_element = review_div.select_one(\"#rptreviews_ctl00_commentspan\")\n",
    "    if fake_element:\n",
    "        fake_status = fake_element.get_text(strip=True)\n",
    "    else:\n",
    "        fake_status = \"No fake status available.\"\n",
    "    return fake_status\n",
    "\n",
    "# Function to extract the domain name (website) from a URL\n",
    "def extract_website(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Function to read the last page number\n",
    "async def get_last_page_number(page):\n",
    "    try:\n",
    "        last_page_selector = \"#spnPaging > li:nth-child(12) > a\"\n",
    "        last_page_element = await page.query_selector(last_page_selector)\n",
    "        if last_page_element:\n",
    "            last_page_text = await last_page_element.inner_text()\n",
    "            return int(last_page_text.strip())\n",
    "        else:\n",
    "            return 1  # Fallback to 1 if last page is not found\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding last page number: {e}\")\n",
    "        return 1\n",
    "\n",
    "# Main extraction function without relying on the \"Next\" button\n",
    "async def extract_reviews(page, base_url, last_page):\n",
    "    review_count = 0\n",
    "    all_reviews = []\n",
    "\n",
    "    for page_number in range(1, last_page + 1):\n",
    "        try:\n",
    "            # Construct the URL for the current page\n",
    "            current_url = f\"{base_url}-page-{page_number}\"\n",
    "            print(f\"Scraping page {page_number} from URL: {current_url}...\")\n",
    "\n",
    "            await page.goto(current_url)\n",
    "            page_source = await page.content()\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Find all review divs with ids matching the pattern\n",
    "            pattern     = re.compile(r'^rptreviews_ctl\\d+_lireviewdetails$')\n",
    "            review_divs = soup.find_all('div', id=pattern)\n",
    "\n",
    "            # Extract data from each review\n",
    "            for review_div in review_divs:\n",
    "                review_text     = extract_review_text(review_div)\n",
    "                star_rating     = extract_star_rating_from_review(review_div)\n",
    "                review_datetime = extract_review_datetime(review_div)\n",
    "                location        = extract_location(review_div)\n",
    "                likes           = extract_likes(review_div)\n",
    "                comments        = extract_comments(review_div)\n",
    "                fake_status     = extract_fake_status(review_div)\n",
    "\n",
    "                all_reviews.append({\n",
    "                    'Rating'       : star_rating,\n",
    "                    'Review Text'  : review_text,\n",
    "                    'Date and Time': review_datetime,\n",
    "                    'Location'     : location,\n",
    "                    'Likes'        : likes,\n",
    "                    'Comments'     : comments,\n",
    "                    'Fake Status'  : fake_status\n",
    "                })\n",
    "\n",
    "            review_count += len(review_divs)\n",
    "\n",
    "            if page_number == last_page:\n",
    "                print(f\"Reached the last page: {last_page}. Exiting.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during review extraction: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Updated run_scraper to pass the current URL\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Load the CSV file with the links\n",
    "        file_path = 'extracted_links.csv'\n",
    "        extracted_links_df = pd.read_csv(file_path)\n",
    "        print(extracted_links_df)\n",
    "\n",
    "        all_data = []\n",
    "        for index, row in extracted_links_df.iterrows():\n",
    "            url = row['Link']\n",
    "            await page.goto(url)\n",
    "\n",
    "            # Get the last page number\n",
    "            last_page = await get_last_page_number(page)\n",
    "            print(f\"Last page number: {last_page}\")\n",
    "\n",
    "            # Extract the base URL (without the page number)\n",
    "            base_url = url.rsplit('-', 1)[0]\n",
    "\n",
    "            # Extract reviews until the last page\n",
    "            extracted_data = await extract_reviews(page, base_url, last_page)\n",
    "\n",
    "            # Extract the website from the URL\n",
    "            website = extract_website(url)\n",
    "\n",
    "            # Add the product name and website to each review\n",
    "            for data in extracted_data:\n",
    "                data['Product'] = row['Text']\n",
    "                data['Website'] = website\n",
    "\n",
    "            all_data.extend(extracted_data)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        reviews_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Save the reviews to a CSV file\n",
    "        reviews_df.to_csv('extracted_reviews.csv', index=False)\n",
    "\n",
    "        await browser.close()\n",
    "        print(\"Reviews extraction complete. Data saved to 'extracted_reviews.csv'.\")\n",
    "\n",
    "# For Jupyter or environments with a running event loop\n",
    "await run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
