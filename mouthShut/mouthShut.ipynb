{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Text                                               Link\n",
      "0  bajaj_avenger  https://www.mouthshut.com/bikes/bajaj-avenger-...\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Next button not found, or no more pages. Exiting.\n",
      "Reviews extraction complete. Data saved to 'extracted_reviews.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to extract star rating from the review_div\n",
    "def extract_star_rating_from_review(review_div):\n",
    "    rating_div = review_div.find('div', class_='rating')\n",
    "    if rating_div:\n",
    "        star_span = rating_div.find('span', recursive=False)\n",
    "        if star_span:\n",
    "            rated_stars = star_span.find_all('i', class_='icon-rating rated-star')\n",
    "            star_rating = len(rated_stars)\n",
    "        else:\n",
    "            star_rating = 0\n",
    "    else:\n",
    "        star_rating = 0\n",
    "    return star_rating\n",
    "\n",
    "# Function to extract review text from the review_div\n",
    "def extract_review_text(review_div):\n",
    "    review_element = review_div.select_one(\"div.more.reviewdata > p\")\n",
    "    if review_element:\n",
    "        review_text = review_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_text = \"No review text available.\"\n",
    "    return review_text\n",
    "\n",
    "# Function to extract date and time from the review_div\n",
    "def extract_review_datetime(review_div):\n",
    "    datetime_element = review_div.find('span', id=re.compile(r'^rptreviews_ctl\\d+_lblDateTime$'))\n",
    "    if datetime_element:\n",
    "        review_datetime = datetime_element.get_text(strip=True)\n",
    "    else:\n",
    "        review_datetime = \"No date and time available.\"\n",
    "    return review_datetime\n",
    "\n",
    "# Function to extract the domain name (website) from a URL\n",
    "def extract_website(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "# Main extraction function with improved \"Next\" button handling\n",
    "async def extract_reviews(page):\n",
    "    review_count = 0\n",
    "    page_number = 1  # Initialize page number\n",
    "    max_reviews = 9999999  # Set your desired number of reviews here\n",
    "    all_reviews = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping page {page_number}...\")  # Print the current page number\n",
    "        \n",
    "        page_source = await page.content()\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find all review divs with ids matching the pattern\n",
    "        pattern = re.compile(r'^rptreviews_ctl\\d+_lireviewdetails$')\n",
    "        review_divs = soup.find_all('div', id=pattern)\n",
    "\n",
    "        # Extract data from each review\n",
    "        for review_div in review_divs:\n",
    "            review_text     = extract_review_text(review_div)\n",
    "            star_rating     = extract_star_rating_from_review(review_div)\n",
    "            review_datetime = extract_review_datetime(review_div)\n",
    "\n",
    "            all_reviews.append({\n",
    "                'Rating'       : star_rating,\n",
    "                'Review Text'  : review_text,\n",
    "                'Date and Time': review_datetime\n",
    "            })\n",
    "\n",
    "        review_count += len(review_divs)\n",
    "\n",
    "        # Stop if review count exceeds max_reviews\n",
    "        if review_count >= max_reviews:\n",
    "            print(\"Desired number of reviews loaded. Exiting.\")\n",
    "            break\n",
    "\n",
    "        # Check for the 'Next' button\n",
    "        next_button = await page.query_selector(\"#litPages > ul > li.next > a\")  # Updated selector\n",
    "        \n",
    "        # Check if the button exists and is not disabled\n",
    "        if next_button:\n",
    "            # Check if the button is disabled (sometimes buttons are disabled but visible)\n",
    "            next_button_disabled = await next_button.get_attribute('class')\n",
    "            if 'disabled' in next_button_disabled:  # Adjust this if the website uses another marker\n",
    "                print(\"Next button is disabled. No more pages to scrape.\")\n",
    "                break\n",
    "\n",
    "            await next_button.click()\n",
    "            await page.wait_for_timeout(2000)  # Adjust wait time if necessary\n",
    "            page_number += 1  # Increment page number after clicking the \"Next\" button\n",
    "        else:\n",
    "            print(\"Next button not found, or no more pages. Exiting.\")\n",
    "            break\n",
    "\n",
    "    return all_reviews\n",
    "\n",
    "# Function to handle scraping logic\n",
    "async def run_scraper():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Load the CSV file with the links\n",
    "        file_path = 'extracted_links.csv'\n",
    "        extracted_links_df = pd.read_csv(file_path)\n",
    "        print(extracted_links_df)\n",
    "\n",
    "        all_data = []\n",
    "        for index, row in extracted_links_df.iterrows():\n",
    "            url = row['Link']\n",
    "            # print(row['Link'])\n",
    "            await page.goto(url)\n",
    "\n",
    "            # Extract reviews\n",
    "            extracted_data = await extract_reviews(page)\n",
    "\n",
    "            # Extract the website from the URL\n",
    "            website = extract_website(url)\n",
    "\n",
    "            # Add the product name and website to each review\n",
    "            for data in extracted_data:\n",
    "                data['Product'] = row['Text']\n",
    "                data['Website'] = website\n",
    "\n",
    "            all_data.extend(extracted_data)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        reviews_df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Save the reviews to a CSV file\n",
    "        reviews_df.to_csv('extracted_reviews.csv', index=False)\n",
    "\n",
    "        await browser.close()\n",
    "        print(\"Reviews extraction complete. Data saved to 'extracted_reviews.csv'.\")\n",
    "\n",
    "# For Jupyter or environments with a running event loop\n",
    "await run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
